{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e056768-166e-4c2c-89ef-53d0af6bde94",
   "metadata": {},
   "source": [
    "# Multiple linear regression model\n",
    "\n",
    "A **multiple linear regression model** is a statistical technique used to predict a dependent variable based on multiple independent variables. It extends simple linear regression by considering multiple predictors.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Formulation\n",
    "\n",
    "The multiple linear regression model with two predictors can be written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\varepsilon}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{y}$ is the vector of dependent variable observations, with size $n \\times 1$, where $n$ is the number of observations (or samples).\n",
    "- $\\mathbf{X}$ is the matrix of independent variables (or regressors), with size $n \\times p$, where $p$ is the number of regressors. In this case, with two regressors (including the intercept term), the matrix will have one column for the intercept (1), one for $x_1$, and one for $x_2$, so $\\mathbf{X}$ will have size $n \\times 3$.\n",
    "- $\\mathbf{\\beta}$ is the vector of coefficients (parameters to estimate), with size $p \\times 1$. In this case, $\\mathbf{\\beta}$ will be a vector of size $3 \\times 1$ (including the intercept term).\n",
    "- $\\mathbf{\\varepsilon}$ is the vector of errors (or residuals), with size $n \\times 1$, representing the difference between the observed and predicted values of the model.\n",
    "\n",
    "For the case with two regressors (including the intercept), the matrix $\\mathbf{X}$ will look like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix} \n",
    "1 & x_{11} & x_{12} \\\\ \n",
    "1 & x_{21} & x_{22} \\\\ \n",
    "\\vdots & \\vdots & \\vdots \\\\ \n",
    "1 & x_{n1} & x_{n2} \n",
    "\\end{bmatrix}_{(n \\times 3)}\n",
    "$$\n",
    "\n",
    "And the vector $\\mathbf{\\beta}$ will be:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\beta} = \\begin{bmatrix} \n",
    "\\beta_0 \\\\ \n",
    "\\beta_1 \\\\ \n",
    "\\beta_2 \n",
    "\\end{bmatrix}_{(3 \\times 1)}\n",
    "$$\n",
    "\n",
    "Thus, the model becomes:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} =\n",
    "\\begin{bmatrix} \n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "\\vdots \n",
    "\\\\ y_n \n",
    "\\end{bmatrix}_{(n \\times 1)} = \n",
    "\\begin{bmatrix} \n",
    "1 & x_{11} & x_{12} \\\\ \n",
    "1 & x_{21} & x_{22} \\\\ \n",
    "\\vdots & \\vdots & \\vdots \\\\ \n",
    "1 & x_{n1} & x_{n2} \n",
    "\\end{bmatrix}_{(n \\times 3)}\n",
    "\\cdot\n",
    "\\begin{bmatrix} \n",
    "\\beta_0 \\\\ \n",
    "\\beta_1 \\\\ \n",
    "\\beta_2 \n",
    "\\end{bmatrix}_{(3 \\times 1)}\n",
    "+\n",
    "\\begin{bmatrix} \n",
    "\\varepsilon_1 \\\\ \n",
    "\\varepsilon_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "\\varepsilon_n \n",
    "\\end{bmatrix}_{(n \\times 1)}\n",
    "$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### OLS estimation \n",
    "\n",
    "The goal is to find the line (or hyperplane in higher dimensions) that best fits the data by minimizing the **sum of squared errors** (cost function).\n",
    "\n",
    "$$\n",
    "\\varepsilon = \\mathbf{y} - \\mathbf{X}\\beta\n",
    "$$\n",
    "\n",
    "The cost function to minimize is the sum of squared errors:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\varepsilon^\\top \\varepsilon = (\\mathbf{y} - \\mathbf{X} \\beta)^\\top (\\mathbf{y} - \\mathbf{X} \\beta)\n",
    "$$\n",
    "\n",
    "To find the minimum of the cost function, it is necessary to calculate the derivative of $ J(\\beta) $ with respect to $ \\beta $ and set it to zero. \n",
    "\n",
    "Expand the cost function:\n",
    "\n",
    "$$\n",
    "J(\\beta) = (\\mathbf{y} - \\mathbf{X} \\beta)^\\top (\\mathbf{y} - \\mathbf{X} \\beta) = \\mathbf{y}^\\top \\mathbf{y} - \\mathbf{y}^\\top \\mathbf{X} \\beta - \\beta^\\top \\mathbf{X}^\\top \\mathbf{y} + \\beta^\\top \\mathbf{X}^\\top \\mathbf{X} \\beta\n",
    "$$\n",
    "\n",
    "Since $ \\mathbf{y}^\\top \\mathbf{X} \\beta $ is a scalar, it follows that $ \\mathbf{y}^\\top \\mathbf{X} \\beta = \\beta^\\top \\mathbf{X}^\\top \\mathbf{y} $. So the cost function becomes:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\mathbf{y}^\\top \\mathbf{y} - 2 \\beta^\\top \\mathbf{X}^\\top \\mathbf{y} + \\beta^\\top \\mathbf{X}^\\top \\mathbf{X} \\beta\n",
    "$$\n",
    "\n",
    "Now calculate the derivative of $ J(\\beta) $ with respect to $ \\beta $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta} = -2 \\mathbf{X}^\\top \\mathbf{y} + 2 \\mathbf{X}^\\top \\mathbf{X} \\beta\n",
    "$$\n",
    "\n",
    "Set the derivative equal to zero to find the estimated coefficients:\n",
    "\n",
    "$$\n",
    "-2 \\mathbf{X}^\\top \\mathbf{y} + 2 \\mathbf{X}^\\top \\mathbf{X} \\beta = 0\n",
    "$$\n",
    "\n",
    "Solve for $ \\beta $:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^\\top \\mathbf{X} \\beta = \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{OLS}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "This is the **Ordinary Least Squares (OLS)** formula for estimating the coefficients $ \\beta $. This vector contains the estimates of $ \\beta_0 $, $ \\beta_1 $, and $ \\beta_2 $, which are the coefficients of the multiple linear regression model.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Goodness of fit measure \n",
    "\n",
    "The coefficient of determination $ R^2 $ is a measure of how well the model fits the data. It is calculated as:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{Sum of Squared Errors (SSE)}}{\\text{Total Sum of Squares (SST)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- The **Total Sum of Squares** (SST) measures the total variability in the data relative to the mean of $ y $:\n",
    "\n",
    "    $$\n",
    "    \\text{SST} = \\sum_{i=1}^{n} {(y_i - \\bar{y})^2}\n",
    "    $$\n",
    "    \n",
    "    with $ \\bar{y} $ being the mean of $ y $.\n",
    "\n",
    "- The **Sum of Squared Errors** (SSE) measures the variability not explained by the model, i.e., the sum of the squared differences between the observed values and the predicted values:\n",
    "\n",
    "    $$\n",
    "    \\text{SSE} = \\sum_{i=1}^{n} {(y_i - \\hat{y}_i)^2}\n",
    "    $$\n",
    "    \n",
    "    where $ \\hat{y}_i $ is the predicted value for $ y_i $.\n",
    "\n",
    "\n",
    "#### Interpretation of $ R^2 $\n",
    "\n",
    "The value of $ R^2 $ ranges from 0 to 1:\n",
    "- $ R^2 = 1 $ means the model perfectly explains the variability in the data.\n",
    "- $ R^2 = 0 $ means the model explains none of the variability in the data.\n",
    "\n",
    "A high $ R^2 $ value indicates that a large portion of the variability in $ y $ is explained by the independent variables in the model, while a low value suggests the model has limited predictive power.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Gaussâ€“Markov assumptions \n",
    "The Gauss-Markov assumptions are a set of conditions under which the Ordinary Least Squares (OLS) estimator $ \\hat{\\beta}_{\\text{OLS}} $ is the Best Linear Unbiased Estimator (BLUE).\n",
    "\n",
    "#### Zero conditional mean \n",
    "The expected value of the residuals conditional on the independent variables should be zero:\n",
    "\n",
    "$$ \\mathbb{E}(\\varepsilon|\\mathbf{X}) = 0 $$\n",
    "\n",
    "This assumption ensures the residuals are random and not linked to the independent variables. In other words, the regressors must be uncorrelated with the residuals i.e. \n",
    "\n",
    "$$ \\text{Cov}(x_i,\\varepsilon_i) = 0 \\quad \\forall i $$\n",
    "\n",
    "#### Homoscedasticity \n",
    "The residuals should have constant variance across all observations.\n",
    "\n",
    "$$ \\text{Var}(\\varepsilon|\\mathbf{X}) = \\sigma \\mathbf{I} $$\n",
    "\n",
    "where $ \\mathbf{I} $ is the identity matrix. This implies that the residuals are equally spread out across all levels of the independent variables.\n",
    "\n",
    "#### No autocorrelation \n",
    "The residuals should be uncorrelated with each other. This means that for all $ i \\neq j $, the covariance between the residuals $ \\varepsilon_i $ and $ \\varepsilon_j $ should be zero: \n",
    "\n",
    "$$ \\text{Cov}(\\varepsilon_i,\\varepsilon_j|\\mathbf{X}) = 0 \\quad i \\neq j $$\n",
    "\n",
    "#### Normality of residuals (optional) \n",
    "The residuals should be normally distributed:\n",
    "\n",
    "$$ \\varepsilon \\sim N(0,\\sigma \\mathbf{I}) $$\n",
    "\n",
    "While this assumption is not necessary for the OLS estimator to be BLUE, it is often assumed for the purpose of hypothesis testing. \n",
    "\n",
    "---\n",
    "\n",
    "# Feed-Forward Neural Network \n",
    "\n",
    "A **Feed-Forward Neural Network (FFNN)** consists of layers of neurons where information flows in one direction: from the input layer through hidden layers and finally to the output layer. Each layer processes the input data by applying a weighted sum, followed by an activation function. For this task, we focus on a network with one hidden layer, using **RReLU** as the activation function.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Network structure\n",
    "\n",
    "1. **Input layer**. The input to the network is a vector $ \\mathbf{x} \\in \\mathbb{R}^n $, where $ n $ is the number of input features.\n",
    "2. **Hidden layer**. This layer consists of $ h $ neurons.\n",
    "3. **Output layer**. The output layer contains $ m $ neurons, and the network produces an output vector $ \\mathbf{y} \\in \\mathbb{R}^m $.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Notation\n",
    "- $ \\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times n} $ is the weight matrix between the input and hidden layers.\n",
    "- $ \\mathbf{b}^{(1)} \\in \\mathbb{R}^h $ is the bias vector for the hidden layer.\n",
    "- $ \\mathbf{W}^{(2)} \\in \\mathbb{R}^{m \\times h} $ is the weight matrix between the hidden and output layers.\n",
    "- $ \\mathbf{b}^{(2)} \\in \\mathbb{R}^m $ is the bias vector for the output layer.\n",
    "- $ \\mathbf{x} \\in \\mathbb{R}^n $ is the input vector.\n",
    "- $ \\mathbf{a}^{(1)} \\in \\mathbb{R}^h $ represents the activations of the hidden layer.\n",
    "- $ \\mathbf{a}^{(2)} \\in \\mathbb{R}^m $ represents the output activations.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Steps \n",
    "\n",
    "#### 1. Input to Hidden Layer\n",
    "\n",
    "The input vector $ \\mathbf{x} $ is multiplied by the weight matrix $ \\mathbf{W}^{(1)} $ and added to the bias vector $ \\mathbf{b}^{(1)} $. This gives the pre-activation values for the hidden layer:\n",
    "\n",
    "$$\n",
    "z_i^{(1)} = \\sum_{j=1}^{n} W_{ij}^{(1)} x_j + b_i^{(1)}, \\quad \\text{for each hidden neuron } i = 1, \\ldots, h\n",
    "$$\n",
    "\n",
    "#### 2. Activation with RReLU\n",
    "\n",
    "The RReLU activation function is applied to the pre-activation values. For each neuron $ i $ in the hidden layer, the output is computed as:\n",
    "\n",
    "$$\n",
    "a_i^{(1)} = \\begin{cases}\n",
    "\\max(0, z_i^{(1)}) & \\text{if } z_i^{(1)} \\geq 0 \\\\\n",
    "\\alpha_i z_i^{(1)} & \\text{if } z_i^{(1)} < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Here, $ \\alpha_i $ is a randomly chosen value from a uniform distribution $ \\alpha_i \\sim \\text{Uniform}(l, u) $, where $ l $ and $ u $ are the lower and upper bounds of the distribution.\n",
    "\n",
    "#### 3. Hidden to Output Layer\n",
    "\n",
    "The activations from the hidden layer $ \\mathbf{a}^{(1)} $ are then multiplied by the weight matrix $ \\mathbf{W}^{(2)} $ and added to the bias vector $ \\mathbf{b}^{(2)} $. This gives the pre-activation for the output layer:\n",
    "\n",
    "$$\n",
    "z_j^{(2)} = \\sum_{i=1}^{h} W_{ji}^{(2)} a_i^{(1)} + b_j^{(2)}, \\quad \\text{for each output neuron } j = 1, \\ldots, m\n",
    "$$\n",
    "\n",
    "#### 4. Output Layer (Final Prediction)\n",
    "\n",
    "The final output of the network is calculated from the pre-activation values of the output layer. If no activation function is applied in the output layer:\n",
    "\n",
    "$$\n",
    "y_j = z_j^{(2)}, \\quad \\text{for each output neuron } j = 1, \\ldots, m\n",
    "$$\n",
    "\n",
    "By following these steps, the network processes input data and makes predictions through forward propagation.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p align=\"center\">Below is the graphical representation of a neural network with 2 input features, 1 hidden layer with 4 neurons, and 1 output.<br /><br />\n",
    "  <img src=\"https://www.researchgate.net/profile/Osama-Mohsen/publication/355094551/figure/fig2/AS:1076573407059968@1633686510430/A-neural-network-with-2-input-features-1-hidden-layer-and-1-output-The-derived-features.png\" alt=\"Rete Neurale\" style=\"width:67%;\"><br /><br />\n",
    "    The derived features of the hidden layer are created from linear combinations of the inputs, and the output is created as a linear combination of these derived features.\n",
    "</p>\n",
    "\n",
    "<br />\n",
    "\n",
    "### Model training and loss tracking \n",
    "\n",
    "The training process, for a regression model, uses **Mean Squared Error (MSE)** as the loss function and **Stochastic Gradient Descent (SGD)** as the optimizer.\n",
    "\n",
    "#### Mean Squared Error (MSE) \n",
    "\n",
    "The MSE loss function measures the average squared differences between actual $ y_i $ and predicted $ \\hat{y_i} $ values:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$ \n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "The model parameters $ \\theta $ are updated using:\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) $$\n",
    "\n",
    "where:\n",
    "- $ \\eta $ is the learning rate\n",
    "- $ \\nabla_{\\theta} J(\\theta) $ is the gradient of the loss function with respect to $ \\theta $.\n",
    "\n",
    "#### Learning rate scheduler \n",
    "The learning rate is adjusted dynamically using `StepLR` scheduler (a powerful tool in PyTorch for adjusting the learning rate during training):\n",
    "\n",
    "$$ \\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor \\frac{t}{\\text{step size}} \\rfloor} $$\n",
    "\n",
    "where:\n",
    "- $ \\eta_0 $ is the initial learning rate\n",
    "- $ \\gamma = 0.5 $ is the decay factor\n",
    "- $ \\lfloor \\frac{t}{\\text{step size}} \\rfloor $ represents the number of completed step intervals.\n",
    "\n",
    "#### Training Loop\n",
    "The model is trained over 200 epochs, performing the following steps in each epoch: \n",
    "\n",
    "1. **Compute predictions**\n",
    "\n",
    "   $$ \\hat{Y} = f(X; \\theta) $$\n",
    "\n",
    "2. **Calculate Loss**\n",
    "\n",
    "   $$ J(\\theta) = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "3. **Compute gradients**\n",
    "\n",
    "   $$ \\nabla_{\\theta} J(\\theta) = \\frac{2}{n} \\sum (y_i - \\hat{y}_i) \\cdot \\frac{\\partial \\hat{y}_i}{\\partial \\theta} $$\n",
    "\n",
    "4. **Update parameters**\n",
    "\n",
    "   $$ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) $$\n",
    "\n",
    "5. **Adjust learning rate**\n",
    "\n",
    "   $$ \\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor \\frac{t}{\\text{step size}} \\rfloor} $$\n",
    "\n",
    "This approach ensures a stable training process by gradually decreasing the learning rate while optimizing model weights using gradient descent.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
